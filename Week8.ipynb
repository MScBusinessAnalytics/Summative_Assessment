{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5b34149a",
      "metadata": {
        "id": "5b34149a"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9b560d5",
      "metadata": {
        "id": "b9b560d5"
      },
      "source": [
        "In this tutorial we are going to begin working through the steps required to build a machine learning model (a \"prediction machine\"). We will apply the predictive analytics methods and aim to build a model which can predict the probability that a company will \"exit\" (i.e. not be active in the future). Then, taking on the role of a client bank, we will then use the model to decide whether to provide the company with a bank loan or not.\n",
        "\n",
        "In this first notebook, we will simply collect some data, design our study sample, and engineer some target features (labels) which we can use to train a machine learning model.\n",
        "\n",
        "Let's begin by importing the pandas package.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c170e8b",
      "metadata": {
        "id": "4c170e8b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "111297cb",
      "metadata": {
        "id": "111297cb"
      },
      "source": [
        "# <center> Data Collection & Preparation</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc7b403",
      "metadata": {
        "id": "cdc7b403"
      },
      "source": [
        "We will be using data taken from the [Predicting firm exit: probability and classification](https://gabors-data-analysis.com/casestudies/#ch17a-predicting-firm-exit-probability-and-classification) case study by [Gabor & Gabor 2021](https://bris.on.worldcat.org/oclc/1250272914). Many of the techniques used in this tutorial are discussed in Chapter 17 of Gabor & Gabor 2021, and further details and background knowledge can be found in this very good book which is available online through the University Library.\n",
        "\n",
        "*Hint*: If you are unfamiliar with a concept, it's worth looking for it in Gabor & Gabor 2021.\n",
        "\n",
        "The dataset will be using is available online from the OSF [website](https://osf.io/b2ft9/). This means we can simply download and import the data using the Pandas `read_csv` function. Let's read in the data and use the `head` method to take a peek."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57d9e00d",
      "metadata": {
        "id": "57d9e00d"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"https://osf.io/download/3qyut/\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edfff5c2",
      "metadata": {
        "id": "edfff5c2"
      },
      "source": [
        "# Sample Design\n",
        "We are not going to use the whole dataset for this study. We are going to attempt to design a study *sample* that aligns with the business problem we want to solve, which can be described as follows:\n",
        "\n",
        "**Business Problem**: It is 1st January 2015, our client is a bank who provides company loans, and they want to build a model to predict whether a company (who is applying for a loan) will default on the bank loan or not.\n",
        "\n",
        "We suggest using 2012 data to build predictors and using 2014 data to approximate the whether a company will default on loan repayments in the future or not. Additionally, the client states they would only consider providing a loan to companies with sales greater than €1000 and less tham €10m.\n",
        "\n",
        "Let's use the Pandas `query` to extract from the the study sample from the dataset and use the `info` method to see what columns are in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76ea17aa",
      "metadata": {
        "id": "76ea17aa"
      },
      "outputs": [],
      "source": [
        "df_sample_2012 = df[(df['year']==2012)|(df['sales']>=1000)|df['sales']<=10000000]\n",
        "df_sample_2012.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4f1b0d6",
      "metadata": {
        "id": "c4f1b0d6"
      },
      "source": [
        "**Note**: Descriptions of the variables names can be found online [here](https://osf.io/9a3t4)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d2fbcef",
      "metadata": {
        "id": "5d2fbcef"
      },
      "source": [
        "# Label Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf971b11",
      "metadata": {
        "id": "bf971b11"
      },
      "source": [
        "Given a lack of labels indicating whether a company defaulted or not, we suggest using future sales data as a *proxy* for whether a company is active or not in the future and, as a result, whether they can repay their loan or not. This process is often called *label engineering* in which we engineer a label approximating the outcome we really want to measure (i.e., whether the company defaulted or not).\n",
        "\n",
        "To do this, we will use data from 2014 to check which companies were active (`sales > 0`) in 2014 and then check which companies in our 2012 sample were *still* active in 2014. We will then use this information to create our (proxy) target `default` label/feature. Finally we will use the `describe` method to inspect our target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00df7423",
      "metadata": {
        "id": "00df7423"
      },
      "outputs": [],
      "source": [
        "# Identify which companies were active (i.e., sales > 0) in 2014\n",
        "active_comps_2014 = df[(df['year']==2014)|df['sales']>0]\n",
        "\n",
        "active_comps_2014_ids = active_comps_2014['comp_id'].values\n",
        "\n",
        "# Assume a company from 2012 will default if it is not active in 2014\n",
        "df_sample_2012['default'] = (~df_sample_2012['comp_id'].isin(active_comps_2014_ids))*1\n",
        "\n",
        "# Describe the target variable\n",
        "print(df_sample_2012['default'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9163a51",
      "metadata": {
        "id": "b9163a51"
      },
      "source": [
        "# Exercise\n",
        "In the code block below, create a study sample which:\n",
        "* Contains observations of companies from 2011 who had current assets greater than €10,000 in 2011\n",
        "* Contains a column (`target`) indicating whether each company still had current assets greater than €10,000 in 2013\n",
        "\n",
        "Then use the `describe` method to answer the following questions:\n",
        "1. How many companies are there in the 2011 sample you created?\n",
        "2. Roughly what % of those companies still had current assets greater than €10,000 in 2013?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6b3e10f",
      "metadata": {
        "id": "f6b3e10f"
      },
      "outputs": [],
      "source": [
        "# (SOLUTION)\n",
        "# create a study sample which:\n",
        "# Contains observations of companies from 2011 who had current assets greater than €10,000 in 2011\n",
        "df_sample_2011 = df.query(\"year==2011 & curr_assets > 10000\")\n",
        "\n",
        "df_sample_2011 = df[(df['year']==2011)&(df['curr_assets']>10000)]\n",
        "\n",
        "comps_2013 = df[(df['year']==2013)&(df['curr_assets']>10000)]['comp_id']\n",
        "\n",
        "\n",
        "# Contains a column (target) indicating whether each company still had current assets greater than €10,000 in 2013\n",
        "df_sample_2011['target'] = df_sample_2011['comp_id'].isin(comps_2013).astype(int)\n",
        "\n",
        "# (Print to check)\n",
        "df_sample_2011.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "085632be",
      "metadata": {
        "id": "085632be"
      },
      "outputs": [],
      "source": [
        "# (SOLUTION)\n",
        "# (Print to check)\n",
        "df_sample_2011['target'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09767d99",
      "metadata": {
        "id": "09767d99"
      },
      "source": [
        "# <center> Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaa77100",
      "metadata": {
        "id": "aaa77100"
      },
      "source": [
        "Below we are going to \"engineer\" some descriptive features which we will use predict to the probability whether a company would default on a loan in the future or not."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6412ac57",
      "metadata": {
        "id": "6412ac57"
      },
      "source": [
        "# Data Collection\n",
        "The sample is publicly available in the GitHub [repo](https://github.com/data-analytics-in-business/gabor-firm-exit-case-study). Therefore we just need to use the `read_csv` method to download and import the data using its URL path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "061825c0",
      "metadata": {
        "id": "061825c0"
      },
      "outputs": [],
      "source": [
        "url = \"https://github.com/data-analytics-in-business/gabor-firm-exit-case-study/raw/main/data/sample_2012.csv\"\n",
        "sample = pd.read_csv(url)\n",
        "sample.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5df3433f",
      "metadata": {
        "id": "5df3433f"
      },
      "source": [
        "We are not going to be using all the columns in the sample to create features. Let's assume we discussed with the client organisation (the bank) and they suggested developing a *baseline* model which uses data relating to sales, profit & loss, and the industry category of the organisation.\n",
        "\n",
        "We will now inspect each of these *domain concepts* and consider how we might use them to engineer features.\n",
        "\n",
        "**Note**: For the benefit of what is to come, we will ensure our target feature `default` is set to be a *string* type object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ba360d1",
      "metadata": {
        "id": "4ba360d1"
      },
      "outputs": [],
      "source": [
        "sample['default'] = sample['default'].astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d7b4829",
      "metadata": {
        "id": "7d7b4829"
      },
      "source": [
        "## Sales\n",
        "Let's first investigate the sales data for the companies in our sample. We will use the `ggplot` function from the `plotnine` [package](https://plotnine.readthedocs.io/en/stable/index.html). Don't worry too much about the details of the plotting package, it's just a tool we use here to quickly create some visualisations for the purpose of investigation.\n",
        "\n",
        "Run the code below to create a plot of the `sales` data from the `sample` to create a histogram plot of the sales data.\n",
        "\n",
        "**Note**: We also use our `default` target feature to split and plot the sales data separately for companies we believe would and would not default in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d2a2a4",
      "metadata": {
        "id": "14d2a2a4"
      },
      "outputs": [],
      "source": [
        "sample['sales'].hist(bins=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2e4365c",
      "metadata": {
        "id": "c2e4365c"
      },
      "source": [
        "We see from the plot that distrobution of the sales data has a very [long tail](https://en.wikipedia.org/wiki/Long_tail). When this is the case, we wil often [*transform*](https://en.wikipedia.org/wiki/Data_transformation_(statistics)) the data before we pass it to a machine learning model. One way to do this (for data > 0) is to perform a [logarithm](https://en.wikipedia.org/wiki/Logarithm) (log) transformation.\n",
        "\n",
        "Let's use the `numpy` package to log-transform our sales data and then visualise it again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b00d63c5",
      "metadata": {
        "id": "b00d63c5"
      },
      "outputs": [],
      "source": [
        "sample['log_sales'] = np.log(sample['sales']+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beaf03fd",
      "metadata": {
        "id": "beaf03fd"
      },
      "outputs": [],
      "source": [
        "sample['log_sales'].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d15ffbe7",
      "metadata": {
        "id": "d15ffbe7"
      },
      "source": [
        "The data looks much more [*normal*](https://en.wikipedia.org/wiki/Normal_distribution) now, and we can even start to see a difference in the distribution of 2012 sales for companies who we believe will default in 2014..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47c0dee7",
      "metadata": {
        "id": "47c0dee7"
      },
      "source": [
        "## Profit and Loss\n",
        "Let's now visualise the distributions of profit (P&L) and loss data in the same way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71b1e6dd",
      "metadata": {
        "id": "71b1e6dd"
      },
      "outputs": [],
      "source": [
        "sample['profit_loss_year'].hist(bins=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bedaa7a8",
      "metadata": {
        "id": "bedaa7a8"
      },
      "source": [
        "The distribution of P&L values also have very long tail(s), but this time they are negative as well as positive. We are going to need to use a slightly more advanced transformation.\n",
        "\n",
        "**Note**: There is a warning above which we can remove by *imputing* the NA values from the `profit_loss_year` column.\n",
        "\n",
        "Run the code below to transform the P&L values, impute the NA values, and visualise the transformed data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57f320fe",
      "metadata": {
        "id": "57f320fe"
      },
      "outputs": [],
      "source": [
        "sample['p+l_scaled'] = np.sign(sample['profit_loss_year'])*(np.log(np.abs(sample['profit_loss_year'])+1))\n",
        "sample['p+l_scaled'] = np.where(sample['p+l_scaled'].isna(), 0, sample['p+l_scaled'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe5e8ff2",
      "metadata": {
        "id": "fe5e8ff2"
      },
      "outputs": [],
      "source": [
        "sample['p+l_scaled'].hist(bins=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "810f0aea",
      "metadata": {
        "id": "810f0aea"
      },
      "source": [
        "The transformed data is not \"normal\" because it is [*bimodal*](https://en.wikipedia.org/wiki/Multimodal_distribution), but it is again much easier to see a difference companies who we believe will and will not default in 2014."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "446c18c9",
      "metadata": {
        "id": "446c18c9"
      },
      "source": [
        "## Industry type\n",
        "Finally, we investigate the industry types of the companies in our sample. We use the `ind` column and create an additional category `0` for companies that have NA in the `ind` column. We also ensure the final `ind_cat` feature is a *string* so that it is not confused to be numeric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b05d2967",
      "metadata": {
        "id": "b05d2967"
      },
      "outputs": [],
      "source": [
        "sample['ind_cat'] = sample['ind']\n",
        "sample['ind_cat'] = np.where(sample['ind_cat'].isna(), 0, sample['ind_cat'])\n",
        "sample['ind_cat'] = sample['ind_cat'].astype(int).astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b065dfe6",
      "metadata": {
        "id": "b065dfe6"
      },
      "outputs": [],
      "source": [
        "sample['ind_cat'].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "500ae567",
      "metadata": {
        "id": "500ae567"
      },
      "source": [
        "# Analytics Base Table\n",
        "Now we have engineered descriptive features based suggested domain concepts, we can use them (along with our target feature) to create our final Analytics Base Table (ABT) which we will use for machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea7b8bbf",
      "metadata": {
        "id": "ea7b8bbf"
      },
      "outputs": [],
      "source": [
        "sample_ABT = sample[['log_sales','p+l_scaled','ind_cat','default']]\n",
        "sample_ABT.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "159e55bf",
      "metadata": {
        "id": "159e55bf"
      },
      "source": [
        "# Exercise\n",
        "In the code block below:\n",
        "1. Transform the `curr_assets` column through a $log(x+1)$ function to create a new column named `log_assets`\n",
        "2. Find the NA values in `log_assets` and impute them using the value `sample['log_assets'].mean()`\n",
        "3. Visualise the distributions of the `log_assets` data for companies who we believe would and would not deafult\n",
        "\n",
        "Based on the visualisation, do companies who would or would not default have more assests of average?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa67f079",
      "metadata": {
        "id": "fa67f079"
      },
      "outputs": [],
      "source": [
        "# (SOLUTION)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f4cec29",
      "metadata": {
        "id": "2f4cec29"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87c198f0",
      "metadata": {
        "id": "87c198f0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "bdcf7d24",
      "metadata": {
        "id": "bdcf7d24"
      },
      "source": [
        "# <center> Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3176c587",
      "metadata": {
        "id": "3176c587"
      },
      "source": [
        "We are going to load some data, preprocess the data so it is in a suitable format for machine learning, and then fit and evaluate a machine learning model. More specifically, we are going to:\n",
        "1. Use the `pandas` package to load our (predictive) Analytics Base Table (ABT) `.csv` data\n",
        "2. Use the `sklearn.preprocessing` package to preprocess the descriptive features in the ABT\n",
        "3. Use the `LogisticRegression` class to train (A.K.A. \"fit\") a logistic regression model\n",
        "4. Use the `sklearn.metrics` package to evaluate the performance of the model\n",
        "\n",
        "Let's begin by loading in the required packages, along with `matplotlib` for creating some visualisations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3376a3f1",
      "metadata": {
        "id": "3376a3f1"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "pd.set_option('mode.chained_assignment',None)\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "849a6f57",
      "metadata": {
        "id": "849a6f57"
      },
      "source": [
        "# Data Collection\n",
        "Let's collect the ABT directly from the previous example"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f58d9988",
      "metadata": {
        "id": "f58d9988"
      },
      "source": [
        "# Data Preprocessing\n",
        "Before we can pass our data to a machine learning algorithm, we need to preprcoess the data so that it is in a suitable format. How we preprocess the data will vary depending on the type of data.\n",
        "\n",
        "First, let's split out ABT into a target feature `y` and descriptive features `X`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f2b2e7f",
      "metadata": {
        "id": "0f2b2e7f"
      },
      "outputs": [],
      "source": [
        "y = sample_ABT['default']\n",
        "X = sample_ABT.drop(columns=['default'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abbfee9f",
      "metadata": {
        "id": "abbfee9f"
      },
      "source": [
        "Now let's split our descriptive features (`X`) into those that are *numeric* (`X_num`) and those that are *categorical* (`X_cat`), and preprocess them accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62bcc10c",
      "metadata": {
        "id": "62bcc10c"
      },
      "outputs": [],
      "source": [
        "arr = np.array([1, 2, 3, 4])\n",
        "reshaped_arr = arr.reshape(-1, 2)\n",
        "reshaped_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c445779e",
      "metadata": {
        "id": "c445779e"
      },
      "outputs": [],
      "source": [
        "X_num = X[['log_sales','p+l_scaled']]\n",
        "X_cat = X['ind_cat'].values.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d988c446",
      "metadata": {
        "id": "d988c446"
      },
      "source": [
        "**Note**: We use the `.reshape(-1, 1)` method to ensure `X_cat` is the correct shape. This is something we need to do when preprocessing a one-dimensional feature array."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a104e7d",
      "metadata": {
        "id": "2a104e7d"
      },
      "source": [
        "## Preprocessing numeric features\n",
        "To preprocess the numeric variables, we are going to use the `MinMaxScaler` [class](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) to scale each numeric feature to be in the range `(0,1)`. Let's create an instance of `MinMaxScaler` and then `fit` and `transform` in one step. Finally, we are going to convert `X_num_scaled` back into a `DataFrame` and `describe` it so we can see the preprocessing step has worked properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84b6740d",
      "metadata": {
        "id": "84b6740d"
      },
      "outputs": [],
      "source": [
        "minmax_scaler = MinMaxScaler()\n",
        "X_num_scaled = minmax_scaler.fit_transform(X_num)\n",
        "X_num_scaled = pd.DataFrame(X_num_scaled, columns=X_num.columns)\n",
        "X_num_scaled.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aa9efaa",
      "metadata": {
        "id": "4aa9efaa"
      },
      "source": [
        "## Preprocessing categorical features\n",
        "To preprocess the categorical variables, we are going to use the `OneHotEncoder` [class](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) to encode each categorical feature as a one-hot numeric array. Let's create an instance of `OneHotEncoder` and then `fit` and `transform` in one step. Finally, we are going to convert `X_cat_onehot` back into a `DataFrame` and print the `head` so we can see the preprocessing step has worked properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "012ed5dd",
      "metadata": {
        "id": "012ed5dd"
      },
      "outputs": [],
      "source": [
        "X_cat_onehot = pd.get_dummies(X['ind_cat'], prefix='ind_cat').drop(columns=['ind_cat_0'])\n",
        "X_cat_onehot.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef7768f4",
      "metadata": {
        "id": "ef7768f4"
      },
      "source": [
        "## Re-joining numeric and categorical features\n",
        "Before we train our model, we are going to re-join our (preprocessed) numeric and categorical features into one `X_preprocessed` array of our (processed) descriptive features. Let's print the `head` to check things look OK.\n",
        "\n",
        "**Note**: We can concatenate (`concat`) them only because the ordering of our rows/examples have not changed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f576208",
      "metadata": {
        "id": "0f576208"
      },
      "outputs": [],
      "source": [
        "X_processed = pd.concat([X_num_scaled, X_cat_onehot], axis=1)\n",
        "X_processed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88a304bc",
      "metadata": {
        "id": "88a304bc"
      },
      "source": [
        "# Fit model\n",
        "Now we have preprocessed our features to a suitable format, we can use `X_processed` and `y` to train a machine learning model. Let's create an instance of the `LogisticRegression` [class](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and `fit` the model to the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb912824",
      "metadata": {
        "id": "eb912824"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X_processed,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d79984d",
      "metadata": {
        "id": "2d79984d"
      },
      "source": [
        "# Evaluate model\n",
        "Let's now assume that we had this model back in 2012 when the descriptive features were collected, ignoring the fact that we wouldn't have had the data to train the model :)\n",
        "\n",
        "Refreshing our memories of our application domain, we want to use the model to predict the likelihood a company will default, and then use those probabilities to decide whether to approve a loan or not (assuming one was applied for).\n",
        "\n",
        "More precisely, we will:\n",
        "1. Use the model to generate an estimate of the probability that each company will default\n",
        "2. Threshold the probability values to decide whether to approve a loan or not\n",
        "3. Compare these decisions to whether the company defaulted in the future or not\n",
        "\n",
        "Run the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df37daa0",
      "metadata": {
        "id": "df37daa0"
      },
      "outputs": [],
      "source": [
        "prob_default = model.predict_proba(X_processed)[:, 1]\n",
        "threshold = 0.5 # Better threshold for loan approval => 0.091\n",
        "loan_approved = (prob_default < threshold)*1\n",
        "company_defaults = [int(x) for x in y]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "047196e7",
      "metadata": {
        "id": "047196e7"
      },
      "outputs": [],
      "source": [
        "ax= plt.subplot()\n",
        "ConfusionMatrixDisplay.from_predictions(company_defaults,loan_approved,ax=ax,display_labels=['no','yes'])\n",
        "ax.set_xlabel('Loan approved')\n",
        "ax.set_ylabel('Company defaults')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b8f03d7",
      "metadata": {
        "id": "9b8f03d7"
      },
      "source": [
        "**Interpretion:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d428dca8",
      "metadata": {
        "id": "d428dca8"
      },
      "source": [
        "High False Positives: In most cases (17,022), the loans were approved and the companies didn’t default. This indicates that the model is effective. However, it’s important to note that these results are based on in-sample testing and may not be entirely reliable. To obtain more robust results, out-of-sample testing is necessary.\n",
        "\n",
        "\n",
        "Relatively Low False Negatives: There are relatively few cases where the model failed to predict an approval for companies that actually defaulted, which is good from a risk management perspective.\n",
        "This confusion matrix indicates the performance of the model and areas for potential improvement, depending on whether you prioritize minimizing false approvals or ensuring approvals for potentially risky clients.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9086097a",
      "metadata": {
        "id": "9086097a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}